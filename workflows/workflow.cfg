###############################################################################
# Workflow configuration file and documentation for ConMan
# © Tom Rainsford, Institut für Linguistik/Romanistik, 2022-
###############################################################################


###############################################################################
# The setup section is used to select up to four objects that are required  #
# for the conversion:                                                         #
#                                                                             #
# - importer : selects the class of importer from conman.importers to use     #
#              for the conversion. If not given, ConMan will attempt to       #
#              use the file extension and will IGNORE the importer sections   # 
#              of this config file.                                           #
# - other_importer :                                                          # 
#              selects the class of importer from conman.importers to use     #
#              loading the concordance to be merged. If not given, ConMan     # 
#              will attempt to use the file extension and will IGNORE the     #
#              other_importer section of this config file.                    #
# - merger :   selects the class of merger from conman.mergers to use to      #
#              merge the concordances. Default is ConcordanceMerger, e.g.     #
#              merge by hit.                                                  #
# - annotator :	                                                              #
#              selects the class of annotator from conman.annotators to use   #
#              once the concordance has been loaded and merged                #
#              The base class Annotator does nothing unless the .script       #
#              method is updated using the advanced settings.                 #
# - exporter : selects the class of exporter from conman.exporters to use for #
#              the conversion. If not given, ConMan will attempt to use the   #
#              file extension and will IGNORE the exporter section of         #
#              this config file.                                              #
###############################################################################
[setup]
importer=PennOutImporter
other_importer=
merger=
annotator=
exporter=TableExporter

###############################################################################
# The importer and other_importer sections contains all parameters used to    #
# configure the concordance importer. Settings are only applied if an         #
# importer is set in the DEFAULT section above.                               # 
#                                                                             #
# Settings related to parsing Tokens (all importers):                         #
# ---------------------------------------------------                         #
# encoding:                                                                   #
#       Character encoding to use for the importer. Overriden in XML-based    #
#       formats by the XML declaration.                                       #
# lcx_regex, keywds_regex, rcx_regex:                                         #
#       Python 3 regex patterns which are used to interpret strings           # 
#       representing the token and eventual tags in the left context,         #
#       the keywords, and the right context respectively. The pattern must    #
#       use symbolic group names using the '(?P<name>...)' operator, and one  #
#       of those groups must be called "word". Other groups are interpreted   #
#       as tags. So, for instance, for token strings in "word_lemma" format,  #
#       the regex should be set to '(?P<word>[^_]+)_(?P<lemma>.*)'.           #
#       Where the tokens are not split into keywords and context, the regex   #
#       in lcx_regex is applied to ALL tokens.                                #
# ref_regex:                                                                  #
#       Python 3 regex pattern used to parse the reference for a hit in the   #
#       concordance and split it into fields. The field names are defined     #
#       using symbolic group names (see above). If not given, importer will   #
#       not attempt to interpret the fields in the reference string.          #
# tokenizer:                                                                  #
#       Selects the tokenizer from conman.tokenizers to be used to tokenize   #
#       the hit. If not passed, defaults to Tokenizer and uses whitespace,    #
#       ignoring all settings in the tokenizer section of this file.          #
#       Also ignored by importers for formats which are already tokenized.    #
#                                                                             #
# Settings related to the TokenListImporter                                   #
# -----------------------------------------                                   #
# TL_hit_end_token:                                                           #
#       Form of the dummy token to use to mark the end of each hit. If not    #
#       given, default is an empty line.                                      #
# TL_comment_string:                                                          #
#		Line-initial string used to comment out sections of an input file.    #
#		Default is '', i.e. no comments.                                      #
#                                                                             #
# Additional settings for the ConllImporter                                   #
# -----------------------------------------                                   #
# The ConllImporter is a subclass of the TokenListImporter with the following #
# additional settings:                                                        #
# CI_head_as_kw:                                                              #
#		If 'True', the head of the parsed sentence will be imported as the kw #
#       If 'False', no keywords will be read. Default is 'True'.              #
#                                                                             #
# Settings for TableImporter (CSV files)                                      #
# --------------------------------------                                      #
# TI_dialect:                                                                 #
#        Dialect to use for the CSV reader. Default: 'excel'. Options are:    #
#       'excel':    Comma-separated, quote with " only when necessary.        #
#        'tab'  :    Tab-separated, no quoting or escaping.                   #
# TI_has_header:                                                              #
#       File has a header row if set to 'True'. Default is True.              #
# TI_fields:                                                                  #
#       Comma-separated list of fields to import in the order in which the    #
#       columns are found in the file. If set, it overrides fieldnames read   #
#       from the header row. The following fieldnames IN CAPITALS must be     #
#       used for the correct import of fields containing the tokens and       #
#       references.                                                           #
#               KEYWORDS:   Keyword tokens only                               #
#               LCX:        Tokens preceding keywords only                    #
#               RCX:        Tokens following keywords only                    #
#               REF:        Reference                                         #
#               TOKENS:     Tokens, if not divided into keywords/context      #
#       Additionally, concordances exported by ConMan can contain the special #
#       field "UUID". This field can be re-imported using the fieldname       # 
#       "UUID" but values should not be modified.                             #
#                                                                             #
# Settings for BaseTreeImporter                                               #
# -------------------------------------------------------------------         #
# BT_keyword_attr:                                                            #
#       Name of the XML attribute which indicates whether the token is a      #
#       keyword or not. Value must be 'yes', 'y', 't' or 'true' (not          # 
#       case-sensitive). If not given, no keywords are identified.            #
#       DO NOT set this if using the PennOutImporter!                         # 
#                                                                             #
# Settings for PennOutImporter                                                #
# ----------------------------                                                #
# PO_keyword_node_regex:                                                      #
#       Python 3 regex used to identify the node number of the keyword node   #
#       from the comment above the tree. The matching node must be identified #
#       by the symbolic group name 'keyword_node'.                            #
#       Default is r':\s*(?P<keyword_node>[0-9]+)\s', i.e. the first node     #
#       after the dominating node.                                            #
#                                                                             # 
###############################################################################

[importer]
lcx_regex=(?P<word>.*)
keywds_regex=(?P<word>.*)
rcx_regex=(?P<word>.*)
TI_dialect=excel
TI_has_header=True
TI_fields=REF,LCX,KEYWORDS,RCX
PO_keyword_node_regex=:\s*[0-9]+\s

[other_importer]
lcx_regex=(?P<word>.*)
keywds_regex=(?P<word>.*)
rcx_regex=(?P<word>.*)
TI_dialect=excel
TI_has_header=True
TI_fields=REF,LCX,KEYWORDS,RCX
PO_keyword_node_regex=:\s*(?P<keyword_node>[0-9]+)\s

###############################################################################
# The exporter section contains all parameters used to                        #
# configure the concordance exporter. Settings are only applied if an         #
# exporter is set in the DEFAULT section above.                               # 
#                                                                             #
# Settings related to representing Tokens (all exporters):                    #
# --------------------------------------------------------                    #
# encoding:                                                                   #
#       Character encoding to use for the exporter.                           #
#                                                                             #
# core_cx:                                                                    #
#		If the CoreContextAnnotator has been run to identify the keywords and #
#       a limited set of tokens surrounding the keyword as the "core context" #
#       setting core_cx to 'True' will export *only* those tokens.            #
#       If no core context has been set, the Exporter will raise an error if  #
#       this is set to True                                                   #
#                                                                             #
# tok_fmt:                                                                    #
#       Format string used to represent each token. Takes a single positional #
#       argument, which is evaluated as a token instance. Default is '{0}',   #
#       i.e. just the token as a string.                                      #                                        
#                                                                             #
# kw_fmt:                                                                     #
#       Format string used to represent each keyword token. Takes a single    #
#       positional argument, which is evaluated as a token instance. Annotation 
#       is accessible using {0.tags[name_of_tag]}. If not passed, uses        #
#       tok_fmt.                                                              #
#                                                                             #
# tok_delimiter:                                                              #
#       String used to delimit the tokens. Default is ' '.                    #
#                                                                             #
# split_hits:                                                                 #
#       Split the exported concordance into a series of separate files        #
#       containing at most split_hits hits. Note that the importers cannot    #
#       handle input from multiple files, so re-importing will either require #
#       file concatenation using the cat command or a series of merges with   #
#       the original concordance using the UUID to match hits. Default is     #
#       not to split.                                                         #
#                                                                             #
# Settings related to the TokenListExporter                                   #
# -----------------------------------------                                   #
# TL_hit_end_token:                                                           #
#       Form of the dummy token to use to mark the end of each hit. If not    #
#       given, default is empty string.                                       #
#                                                                             #
# Settings related to the TableExporter                                       #
# -------------------------------------                                       #
# TE_dialect:                                                                 #
#       Dialect to use for the CSV writer. Default: 'excel'. Options are:     #
#       'excel':    Comma-separated, quote with " only when necessary.        #
#       'tab'  :    Tab-separated, no quoting or escaping.                    #
#                                                                             #
# TE_header:                                                                  #
#       Write a header row if set to "True". Default is True.                 #
#                                                                             #
# TE_fields:                                                                  #
#       Comma-separated list of fields to export in the order in which the    #
#       columns are found in the file. The following fieldnames IN CAPITALS   #
#       must be used for the correct export of fields containing the tokens   #   
#       and references.                                                       #
#               KEYWORDS:   Keyword tokens only                               #
#               LCX:        Tokens preceding keywords only                    #
#               RCX:        Tokens following keywords only                    #
#               REF:        Reference                                         #
#               TOKENS:     Tokens, if not divided into keywords/context      #
#               UUID:       A unique ID. Essential if annotation will be added#
#                           and re-imported.                                  #
#                                                                             #
# Settings related to the ConllExporter                                       #
# -------------------------------------                                       #
# CE_lemma, CE_cpostag, CE_postag, CE_head, CE_deprel, CE_phead, CE_pdeprel:  #
#       Name of token tag to be written to each column in the Conll file.     #
#       Default values are 'conll_LEMMA', 'conll_CPOSTAG', etc.               #
# CE_feats:                                                                   #
#       List of token tag names to be exported as key=value pairs in the      #
#       FEATS column of the Conll file. If not given, no feats are written.   #
# CE_hit_end_token:                                                           #
#       Form of the dummy token to use to mark the end of each hit. If not    #
#       given, default is the string 'ENDHIT'                                 #
###############################################################################

[exporter]
tok_fmt={0}
tok_delimiter= 
TE_dialect=excel
TE_header=True
CE_lemma=conll_LEMMA
CE_cpostag=conll_CPOSTAG
CE_postag=conll_POSTAG
CE_head=conll_HEAD
CE_deprel=conll_DEPREL
CE_phead=conll_PHEAD
CE_pdeprel=conll_PDEPREL
CE_split_hit=False

###############################################################################
# The merger section contains all parameters used when merging two            #
# concordances. The ConcordanceMerger matches hits to hits and should normally#
# be used. However, if the division into hits is not the same, as can happen  #
# when the data has been parsed or tagged by an external tool, the            #
# TextMerger will treat the concordances as texts, i.e. lists of tokens, and  #
# will run the TTA to match them token-by-token. The division into hits of    #
# primary concordance will be retained.                                       #
#                                                                             #
# Settings related to the ConcordanceMerger                                   #
# -----------------------------------------                                   #
#                                                                             #
# CM_add_hits:                                                                #
#       If True, adds extra hits to the concordance, otherwise extra hits are #
#       ignored.                                                              #
# CM_del_hits:                                                                #
#       If True, deletes all hits in concordance that are not in the other    #
#       concordance.                                                          #
# CM_core_cx:                                                                 #
#       If True, assumes that the secondary concordance contains only those   #
#       tokens in the core context of the primary concordance. Only relevant  #
#       if CM_merge_tokens is set to True. Default is False.                  #
# CM_match_by:                                                                #
#       A string instructing the merger how to match hits. The only possible  #
#       values are 'uuid' and 'ref'. If not passed (default), assumes that    #
#       the two concordances contain the same hits in the same order, i.e.    #
#       in cases where only some new annotation has been added.               #
# CM_update_hit_tags:                                                         #
#       If True, update hit-level tags which are already present in the main  #
#       concordance with new values from the other concordance. If False,     #
#       hit-level are not updated and only new tags are added.                #
# CM_merge_tokens, CM_update_token_tags:                                      #
#       merge_tokens=True:                                                    #
#           add new token-level tags from the other concordance.              #
#       merge_tokens=True, update_token_tags=True:                            #
#           also update existing token-level tags in the main concordance     #
#           using the values from the other concordance.                      #
# CM_tok_id_tag:                                                              #
#       The name of the tag giving a unique ID for each token within the hit  #
#       If not given, assumes that corresponding hits in the two concordances #
#       contain the same tokens in the same order.                            #
#                                                                             #
# Settings related to the TextMerger                                          #
# ----------------------------------                                          #
#                                                                             #
# TM_core_cx:                                                                 #
#       If True, assumes that the secondary concordance contains only those   #
#       tokens in the core context of the primary concordance. Default is     #
#       False.                                                                #
#       If this is not set correctly, the similarity ratio between the texts  #
#       will almost certainly be too low and the aligner will abort.          #
# TM_hit_end_token:                                                           #
#		The aligner is very slow on long texts, so if the original division   #
#       into hits can still be detected, it is advantageous to parse hit by   #
#       hit. The hit_end_token instructs the aligner to parse hit by hit.     #
# TM_threshold:                                                               #
#       Threshold value for the TTA, see TTA documentation. Default is 20.    #
# TM_ratio:                                                                   #
#       Minimum similarity ratio for the TTA, see TTA documentation.          #
#       Default is .95.                                                       #
###############################################################################
[merger]

###############################################################################
# The annotator section contains the keyword arguments to be passed to the    #
# annotator's script method. All values are evaluated directly as Python      #
# code, e.g. tags=[('lemma_lgerm', 'lemma')] will produce a list containing a #
# two-tuple of two strings.                                                   #
###############################################################################
[annotator]
tags=[('lemma', 'lemma_bfm'), ('lemma_lgerm', 'lemma_lgerm')]

################################################################################
# Settings in the advanced section of the file should only be altered if you   #
# understand the object attributes in importers.py that are affected!          #
################################################################################
[advanced]
#PO_dump_xml=tmp.xml
#PO_script_file=conman/scripts/pennout2cnc.py
#annotator_script_file=