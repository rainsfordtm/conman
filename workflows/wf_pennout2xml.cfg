###############################################################################
# Workflow configuration file and documentation for ConMan
# © Tom Rainsford, Institut für Linguistik/Romanistik, 2022-
###############################################################################

###############################################################################
# The purpose of this workflow file is output only the XML conversion of the  #
# Penn .out file, originally designed as a debugging option. This is so that  #
# the results of the .out file can then be further processed by other tools   #
# such as XSLT stylesheet transformations, into a more complex,               #
# syntax-focused tabular format.                                              #
###############################################################################

###############################################################################
# The setup section is used to select up to four objects that are required  #
# for the conversion:                                                         #
#                                                                             #
# - importer : selects the class of importer from conman.importers to use     #
#              for the conversion. If not given, ConMan will attempt to       #
#              use the file extension and will IGNORE the importer sections   # 
#              of this config file.                                           #
# - other_importer :                                                          # 
#              selects the class of importer from conman.importers to use     #
#              loading the concordance to be merged. If not given, ConMan     # 
#              will attempt to use the file extension and will IGNORE the     #
#              other_importer section of this config file.                    #
# - annotator :	                                                              #
#              selects the class of annotator from conman.annotators to use   #
#              once the concordance has been loaded and merged                #
#              The base class Annotator does nothing unless the .script       #
#              method is updated using the advanced settings.                 #
# - exporter : selects the class of exporter from conman.exporters to use for #
#              the conversion. If not given, ConMan will attempt to use the   #
#              file extension and will IGNORE the exporter section of         #
#              this config file.                                              #
###############################################################################
[setup]
importer=PennOutImporter
other_importer=
annotator=
exporter=

###############################################################################
# The importer and other_importer sections contains all parameters used to    #
# configure the concordance importer. Settings are only applied if an         #
# importer is set in the DEFAULT section above.                               # 
#                                                                             #
# Settings related to parsing Tokens (all importers):                         #
# ---------------------------------------------------                         #
# encoding:                                                                   #
#       Character encoding to use for the importer. Overriden in XML-based    #
#       formats by the XML declaration.                                       #
# lcx_regex, keywds_regex, rcx_regex:                                         #
#       Python 3 regex patterns which are used to interpret strings           # 
#       representing the token and eventual tags in the left context,         #
#       the keywords, and the right context respectively. The pattern must    #
#       use symbolic group names using the '(?P<name>...)' operator, and one  #
#       of those groups must be called "word". Other groups are interpreted   #
#       as tags. So, for instance, for token strings in "word_lemma" format,  #
#       the regex should be set to '(?P<word>[^_]+)_(?P<lemma>.*)'.           #
#       Where the tokens are not split into keywords and context, the regex   #
#       in lcx_regex is applied to ALL tokens.                                #
# ref_regex:                                                                  #
#       Python 3 regex pattern used to parse the reference for a hit in the   #
#       concordance and split it into fields. The field names are defined     #
#       using symbolic group names (see above). If not given, importer will   #
#       not attempt to interpret the fields in the reference string.          #
# tokenizer:                                                                  #
#       Selects the tokenizer from conman.tokenizers to be used to tokenize   #
#       the hit. If not passed, defaults to Tokenizer and uses whitespace,    #
#       ignoring all settings in the tokenizer section of this file.          #
#       Also ignored by importers for formats which are already tokenized.    #
#                                                                             #
# Settings related to the TokenListImporter
# -----------------------------------------                                   #
# TL_hit_end_token:                                                           #
#       Form of the dummy token to use to mark the end of each hit. If not    #
#       given, default is empty string.                                       #
#                                                                             #
# Settings for TableImporter (CSV files)                                      #
# --------------------------------------                                      #
# TI_dialect:                                                                 #
#        Dialect to use for the CSV reader. Default: 'excel'. Options are:    #
#       'excel':    Comma-separated, quote with " only when necessary.        #
#        'tab'  :    Tab-separated, no quoting or escaping.                   #
# TI_has_header:                                                              #
#       File has a header row if set to 'True'. Default is True.              #
# TI_fields:                                                                  #
#       Comma-separated list of fields to import in the order in which the    #
#       columns are found in the file. If set, it overrides fieldnames read   #
#       from the header row. The following fieldnames IN CAPITALS must be     #
#       used for the correct import of fields containing the tokens and       #
#       references.                                                           #
#               KEYWORDS:   Keyword tokens only                               #
#               LCX:        Tokens preceding keywords only                    #
#               RCX:        Tokens following keywords only                    #
#               REF:        Reference                                         #
#               TOKENS:     Tokens, if not divided into keywords/context      #
#       Additionally, concordances exported by ConMan can contain the special #
#       field "UUID". This field can be re-imported using the fieldname       # 
#       "UUID" but values should not be modified.                             #
#                                                                             #
# Settings for BaseTreeImporter                                               #
# -------------------------------------------------------------------         #
# BT_keyword_attr:                                                            #
#       Name of the XML attribute which indicates whether the token is a      #
#       keyword or not. Value must be 'yes', 'y', 't' or 'true' (not          # 
#       case-sensitive). If not given, no keywords are identified.            #
#       DO NOT set this if using the PennOutImporter!                         # 
#                                                                             #
# Settings for PennOutImporter                                                #
# ----------------------------                                                #
# PO_keyword_node_regex:                                                      #
#       Python 3 regex used to identify the node number of the keyword node   #
#       from the comment above the tree. The matching node must be identified #
#       by the symbolic group name 'keyword_node'.                            #
#       Default is r'[^0-9]*(?P<keyword_node>[0-9]+)[^0-9]+.*', i.e. the first#
#       number in the comment (typically the dominating IP).                  #
#                                                                             # 
###############################################################################

[importer]
PO_keyword_node_regex=,\s+(?P<keyword_node>[0-9]+)\s+V[^,]*,

[other_importer]

###############################################################################
# The exporter section contains all parameters used to                        #
# configure the concordance exporter. Settings are only applied if an         #
# exporter is set in the DEFAULT section above.                               # 
#                                                                             #
# Settings related to representing Tokens (all exporters):                    #
# --------------------------------------------------------                    #
# encoding:                                                                   #
#       Character encoding to use for the exporter.                           #
#                                                                             #
# tok_fmt:                                                                    #
#       Format string used to represent each token. Takes a single positional #
#       argument, which is evaluated as a token instance. Default is '{0}',   #
#       i.e. just the token as a string.                                      #                                        
#                                                                             #
# kw_fmt:                                                                     #
#       Format string used to represent each keyword token. Takes a single    #
#       positional argument, which is evaluated as a token instance. Annotation 
#       is accessible using {0.tags[name_of_tag]}. If not passed, uses        #
#       tok_fmt.                                                              #
#                                                                             #
# tok_delimiter:                                                              #
#       String used to delimit the tokens. Default is ' '.                    #
#                                                                             #
# split_hits:                                                                 #
#       Split the exported concordance into a series of separate files        #
#       containing at most split_hits hits. Note that the importers cannot    #
#       handle input from multiple files, so re-importing will either require #
#       file concatenation using the cat command or a series of merges with   #
#       the original concordance using the UUID to match hits. Default is     #
#       not to split.                                                         #
#                                                                             #
# Settings related to the TokenListExporter                                   #
# -----------------------------------------                                   #
# TL_hit_end_token:                                                           #
#       Form of the dummy token to use to mark the end of each hit. If not    #
#       given, default is empty string.                                       #
#                                                                             #
# Settings related to the TableExporter                                       #
# -------------------------------------                                       #
# TE_dialect:                                                                 #
#       Dialect to use for the CSV writer. Default: 'excel'. Options are:     #
#       'excel':    Comma-separated, quote with " only when necessary.        #
#       'tab'  :    Tab-separated, no quoting or escaping.                    #
#                                                                             #
# TE_header:                                                                  #
#       Write a header row if set to "True". Default is True.                 #
#                                                                             #
# TE_fields:                                                                  #
#       Comma-separated list of fields to export in the order in which the    #
#       columns are found in the file. The following fieldnames IN CAPITALS   #
#       must be used for the correct export of fields containing the tokens   #   
#       and references.                                                       #
#               KEYWORDS:   Keyword tokens only                               #
#               LCX:        Tokens preceding keywords only                    #
#               RCX:        Tokens following keywords only                    #
#               REF:        Reference                                         #
#               TOKENS:     Tokens, if not divided into keywords/context      #
#               UUID:       A unique ID. Essential if annotation will be added#
#                           and re-imported.                                  #
#                                                                             #
# Settings related to the ConllExporter                                       #
# -------------------------------------                                       #
# CE_lemma, CE_cpostag, CE_postag, CE_head, CE_deprel, CE_phead, CE_pdeprel:  #
#       Name of token tag to be written to each column in the Conll file.     #
#       Default values are 'conll_LEMMA', 'conll_CPOSTAG', etc.               #
# CE_feats:                                                                   #
#       List of token tag names to be exported as key=value pairs in the      #
#       FEATS column of the Conll file. If not given, no feats are written.   #
###############################################################################

[exporter]


###############################################################################
# The merger section contains all parameters used when merging two            #
# concordances.                                                               #
#                                                                             #
# add_hits:                                                                   #
#       If True, adds extra hits to the concordance, otherwise extra hits are #
#       ignored.                                                              #
# del_hits:                                                                   #
#       If True, deletes all hits in concordance that are not in the other    #
#       concordance.                                                          #
# match_by:                                                                   #
#       A string instructing the merger how to match hits. The only possible  #
#       values are 'uuid' and 'ref'. If not passed (default), assumes that    #
#       the two concordances contain the same hits in the same order, i.e.    #
#       in cases where only some new annotation has been added.               #
# update_hit_tags:                                                            #
#       If True, update hit-level tags which are already present in the main  #
#       concordance with new values from the other concordance. If False,     #
#       hit-level are not updated and only new tags are added.                #
# merge_tokens, update_token_tags:                                            #
#       merge_tokens=True:                                                    #
#           add new token-level tags from the other concordance.              #
#       merge_tokens=True, update_token_tags=True:                            #
#           also update existing token-level tags in the main concordance     #
#           using the values from the other concordance.                      #
# tok_id_tag:                                                                 #
#       The name of the tag giving a unique ID for each token within the hit  #
#       If not given, assumes that corresponding hits in the two concordances #
#       contain the same tokens in the same order.                            #
###############################################################################
[merger]

###############################################################################
# The annotator section contains the keyword arguments to be passed to the    #
# annotator's script method. All values are evaluated directly as Python      #
# code, e.g. tags=[('lemma_lgerm', 'lemma')] will produce a list containing a #
# two-tuple of two strings.                                                   #
###############################################################################
[annotator]

################################################################################
# Settings in the advanced section of the file should only be altered if you   #
# understand the object attributes in importers.py that are affected!          #
################################################################################
[advanced]
PO_dump_xml=out.xml
#PO_script_file=conman/scripts/pennout2cnc.py
#annotator_script_file=
